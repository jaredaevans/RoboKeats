{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keats generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -q -U tensorflow-addons\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "#if not tf.config.list_physical_devices('GPU'):\n",
    "#    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "#    if IS_COLAB:\n",
    "#        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning functions:\n",
    "def deRomanNumeral(words): #note: doesn't remove roman numeral I, as that is used often...\n",
    "    pattern = '^(cm?|cd?|d?c?c?c?)(xc?|xl?|l?x?x?x?)(ix|iv|v?i?ii)$'\n",
    "    words = [ w for w in words if re.search(pattern,w) is not None ]\n",
    "    return words\n",
    "\n",
    "# Text cleaning functions:\n",
    "def checkRomanNumeral(word): #note: doesn't remove roman numeral I, as that is used often...\n",
    "    if word == 'i':\n",
    "        return None\n",
    "    pattern = '^(cm?|cd?|d?c?c?c?)(xc?|xl?|l?x?x?x?)(m|d|c|x|v|l|ix|iv|v?i?i?i)$'\n",
    "    return re.search(pattern,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newlinepartition(text):\n",
    "    # cut up a single entry\n",
    "    septext = text.split(\"\\n\\n\\n\")\n",
    "    if len(septext) == 2 and len(septext[0]) < 200:\n",
    "        return septext[1]\n",
    "    if len(septext) > 2:\n",
    "        newtext = []\n",
    "        for tt in septext:\n",
    "            if len(tt) < 100:\n",
    "                pass\n",
    "            elif tt[:9]=='FOOTNOTES':\n",
    "                print(\"removed footnotes\")\n",
    "                pass\n",
    "            elif tt[:9]=='LINENOTES':\n",
    "                print(\"removed linenotes\")\n",
    "                pass\n",
    "            elif tt[:9]=='NOTE':\n",
    "                print(\"removed note\")\n",
    "                pass\n",
    "            else:\n",
    "                newtext.append(tt)\n",
    "        if(len(newtext)>1):\n",
    "            print('check this text')\n",
    "        return \"\\n\\n\".join(newtext)\n",
    "    if len(septext) < 2:\n",
    "        print(\"assumed empty\")\n",
    "        return None\n",
    "    \n",
    "def cleanup_text_A(text): # Keats # Coleridge\n",
    "    words = text.replace(\"\\n\\n\",\"\\n\")\n",
    "    words = words.replace(\"\\n\",\" \\n \")\n",
    "    words = words.replace(\"-\",\" \")\n",
    "    words = words.replace(\".\",\" . \")\n",
    "    words = words.replace(\"!\",\" ! \")\n",
    "    words = words.replace(\"?\",\" ? \")\n",
    "    words = words.split(\" \")\n",
    "    table = str.maketrans('', '', '…”_-.,;!:?*/()[]{}0123456789\"')\n",
    "    stripped = [w.translate(table).lower() for w in words]\n",
    "    stripped = [w for w in stripped if not w=='' and not checkRomanNumeral(w)]\n",
    "    cleanedtext = \" \".join(stripped)\n",
    "    cleanedtext = cleanedtext.replace(\"\\n part \\n\",\"\\n\")\n",
    "    cleanedtext = cleanedtext.replace(\"part i \\n\",\"\")\n",
    "    cleanedtext = cleanedtext.replace(\"\\n book \\n\",\"\\n\")\n",
    "    cleanedtext = cleanedtext.replace(\"book i \\n\",\"\")\n",
    "    cleanedtext = cleanedtext.replace(\"\\n scene \\n\",\"\\n\")\n",
    "    cleanedtext = cleanedtext.replace(\"scene i \\n\",\"\")\n",
    "    cleanedtext = cleanedtext.replace(\"\\n \\n\",\"\\n\")\n",
    "    cleanedtext = cleanedtext.replace(\"\\n note \\n\", 'TERMINATE')\n",
    "    cleanedtext = cleanedtext.replace(\"\\n notes \\n\", 'TERMINATE')\n",
    "    cleanedtext = cleanedtext.replace(\"\\n footnote \\n\", 'TERMINATE')\n",
    "    cleanedtext = cleanedtext.replace(\"\\n footnotes \\n\", 'TERMINATE')\n",
    "    cleanedtext = cleanedtext.split('TERMINATE')[0]\n",
    "    cleanedtext = re.sub(\"(\\n |)verse (st|nd|rd|th) \\n\",\"\\n\", cleanedtext)\n",
    "    cleanedtext = cleanedtext.lstrip(\"i \")\n",
    "    cleanedtext = cleanedtext.split(\" ed \\n\")[-1] # remove editors notes in wordsworth (may be dangerous)\n",
    "    cleanedtext = cleanedtext.replace(\"composed published \",\"\")\n",
    "    cleanedtext = cleanedtext.lstrip(\"\\n\")\n",
    "    return cleanedtext\n",
    "    \n",
    "def split_into_works(maintext,iStart,iEnd, type='A'):\n",
    "    # first identify where legal / forewardend etc, set these to iStart and iEnd (with a +!)\n",
    "    textlist = []\n",
    "    for i in range(iStart,iEnd):\n",
    "        print(i)\n",
    "        text = newlinepartition(maintext[i])\n",
    "        if text is not None: \n",
    "            if type == 'A' or type == '-':\n",
    "                textlist.append(cleanup_text_A(text))\n",
    "            elif type == 'B':\n",
    "                textlist.append(cleanup_text_B(text))\n",
    "    return textlist\n",
    "\n",
    "def split_works_into_words(works):\n",
    "    return [l.split(\" \") for l in works ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keats test (Blake's poems not split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keatsfilelist = [ file for file in os.listdir() if file[-4:]=='.txt' and file[:5]=='Keats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedictionary = {\n",
    "                  'Keats_1.txt': {'zones': 2, 'len': [8, 22], 'cleanup': 'A'},\n",
    "                  'Keats_2.txt':  {'zones': 2, 'len': [5, 37],'cleanup': 'A'},\n",
    "                  'Keats_3.txt': {'zones': 2, 'len': [6, 10], 'cleanup': 'A'},\n",
    "                  'Keats_4.txt': {'zones': 2, 'len': [0, 0], 'cleanup': 'A'}, #This is only Lamia which is in Keats_1.txt\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_process_file(filelist,idx):\n",
    "    filename = filelist[idx]\n",
    "    file = open(filename, 'rt')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    adict = filedictionary[filename]\n",
    "    # split into words by white space\n",
    "    Zones = text.split(\"***\")\n",
    "    lens = adict['len']\n",
    "    maintext = []\n",
    "    if adict['zones'] == 'asmain':\n",
    "        maintext = Zones\n",
    "    else:\n",
    "        maintext = Zones[adict['zones']].split(\"\\n\\n\\n\\n\\n\")\n",
    "    maintext = split_into_works(maintext,lens[0],lens[1],type=adict['cleanup'])\n",
    "    return maintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "check this text\n",
      "13\n",
      "14\n",
      "15\n",
      "assumed empty\n",
      "16\n",
      "17\n",
      "assumed empty\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "check this text\n",
      "36\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "8\n",
      "removed footnotes\n",
      "check this text\n",
      "9\n",
      "10\n",
      "11\n",
      "assumed empty\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "check this text\n"
     ]
    }
   ],
   "source": [
    "workslist = []\n",
    "for i in range(len(keatsfilelist)): \n",
    "    workslist+= read_and_process_file(keatsfilelist,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A keats text run (based on char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this allows to smooth out a distrubtion and equalize probabilities ()\n",
    "def reweight_distribution(orig, temp=1.,eps=1e-8):\n",
    "    new = np.exp(np.log(np.abs(orig)+eps) / temp)\n",
    "    return new / np.sum(new)\n",
    "\n",
    "def sample(preds, temp = 1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = reweight_distribution(preds, temp)\n",
    "    probs = np.random.multinomial(1,preds,1)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "corpus = \"\\n\\n\".join(workslist)\n",
    "len(corpus)\n",
    "maxlen = 80\n",
    "allchars = sorted(list(set(corpus)))\n",
    "charind = dict((char,allchars.index(char)) for char in allchars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Sequences: 369141\n",
      "# chars: 36\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "sequences = []\n",
    "next_char = []\n",
    "\n",
    "for i in range(0,len(corpus)-maxlen,step):\n",
    "    sequences.append(corpus[i:i+maxlen])\n",
    "    next_char.append(corpus[i+maxlen])\n",
    "    \n",
    "print(\"# Sequences: \"+str(len(sequences)))\n",
    "print(\"# chars: \"+str(len(allchars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize all values as zero\n",
    "x = np.zeros((len(sequences),maxlen,len(allchars)),dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(allchars)),dtype=np.bool)\n",
    "for i, seq in enumerate(sequences):\n",
    "    for j, char in enumerate(seq):\n",
    "        x[i,j,charind[char]] = 1 # one hot encode\n",
    "    y[i,charind[next_char[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keatsmodel = Sequential()\n",
    "keatsmodel.add(LSTM(128,input_shape=(maxlen,len(allchars))))\n",
    "keatsmodel.add(Dense(len(allchars),activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = RMSprop(lr=0.01)\n",
    "keatsmodel.compile(loss=\"categorical_crossentropy\",optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Train on 369141 samples\n",
      "Epoch 1/3\n",
      "369141/369141 [==============================] - 1596s 4ms/sample - loss: 1.3808\n",
      "Epoch 2/3\n",
      "369141/369141 [==============================] - 1536s 4ms/sample - loss: 1.3728\n",
      "Epoch 3/3\n",
      "369141/369141 [==============================] - 1547s 4ms/sample - loss: 1.3652\n",
      "--- temperature:  0.25\n",
      "bee hive casts its swarm \n",
      " acorns ripe down pattering \n",
      " while the autumn breezes of the brint \n",
      " the bright and the clouds in many a sorrow \n",
      " the soft and startled the mid and so \n",
      " and for the bright with the bright and the bowery \n",
      " of some soul to see the shade and seem \n",
      " and the--- temperature:  0.5\n",
      "ith the bright and the bowery \n",
      " of some soul to see the shade and seem \n",
      " and the courted wand the fire upon her wood \n",
      " and morn to light for the pout thevery \n",
      " as a some morning happy with things \n",
      " of all the white we start with beauty bed \n",
      " of her rich thee be a spring side \n",
      " to--- temperature:  0.75\n",
      " all the white we start with beauty bed \n",
      " of her rich thee be a spring side \n",
      " to that then her maning sung \n",
      " to could high affedden eyes \n",
      " old sorrow changers he must spine \n",
      " but from its worled and the 'twaster burn \n",
      " a give i silence greet of her winger \n",
      " the deep child as a la--- temperature:  1.0\n",
      "he 'twaster burn \n",
      " a give i silence greet of her winger \n",
      " the deep child as a lap to each dull to thing \n",
      " meet silently a allous brow the ear \n",
      " fre come haves and thine own hand fance and burst \n",
      " e'en bentherce thou sweet whose beauty pollar \n",
      " spirits four s every his dear leaf \n",
      "--- temperature:  1.25\n",
      " bentherce thou sweet whose beauty pollar \n",
      " spirits four s every his dear leaf \n",
      "abbe lefter bevered in prese still \n",
      " lightwaming i've be alo out i shall be ksuset \n",
      " the wide ere treth track wischishing aslay'd \n",
      " she snootless birth sake thaced latce begin \n",
      " to faled airy cavelinkEpoch:  3\n",
      "Train on 369141 samples\n",
      "Epoch 1/3\n",
      "369141/369141 [==============================] - 1641s 4ms/sample - loss: 1.3590\n",
      "Epoch 2/3\n",
      "369141/369141 [==============================] - 1489s 4ms/sample - loss: 1.3550\n",
      "Epoch 3/3\n",
      "369141/369141 [==============================] - 1448s 4ms/sample - loss: 1.3500\n",
      "--- temperature:  0.25\n",
      "ll belong \n",
      " to speak o turn thee to the very tale \n",
      " and taste the music of that sat end \n",
      " and the thousand in the bright beauty \n",
      " the stars of the morning soft and the stream \n",
      " and while the streams and seem'd the dead \n",
      " and the strings and like a spiritual string \n",
      " and in the pa--- temperature:  0.5\n",
      "nd seem'd the dead \n",
      " and the strings and like a spiritual string \n",
      " and in the passion yet shall be song \n",
      " and round the look'd and arms and half fair dead \n",
      " than a spirits dainty of the shaded \n",
      " before he of the temple speak full \n",
      " whose heartion fearful stars of gradual \n",
      " of min--- temperature:  0.75\n",
      "e he of the temple speak full \n",
      " whose heartion fearful stars of gradual \n",
      " of mingled throning fair'd her mample with her blooms \n",
      " and burred awhile shone me stand and thear sleep \n",
      " a smart was a brimm'd and maid the clear \n",
      " so den shourst thou were heart commudes star \n",
      " is their --- temperature:  1.0\n",
      "'d and maid the clear \n",
      " so den shourst thou were heart commudes star \n",
      " is their blushing but an  i''ll see mought a\n",
      " day but hided child echihing to and \n",
      " moon thou mien ureyow is paths my porphyrour \n",
      " a well i am about the stars of fans \n",
      " toes a rance me he is to ere porward ben--- temperature:  1.25\n",
      "\n",
      " a well i am about the stars of fans \n",
      " toes a rance me he is to ere porward bend \n",
      " be whisper ditay'rs madlaious cirly lone \n",
      " while thou seest now porfach tustlew streek \n",
      " guir'd those barixerian's ffroremest 'twild ost might \n",
      " to now the raon'd winders cheekly groan dead \n",
      " if d"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    print('Epoch: ',3*epoch)\n",
    "    keatsmodel.fit(x,y,batch_size=128,epochs = 3) # run for just three epochs\n",
    "    \n",
    "    start_ind = np.random.randint(0,len(corpus)-maxlen-1)\n",
    "    gen_text = corpus[start_ind:start_ind+maxlen]\n",
    "    \n",
    "    for temp in [0.25,0.5,0.75,1.0,1.25]:\n",
    "        print(\"--- temperature: \",temp)\n",
    "        sys.stdout.write(gen_text)\n",
    "        for i in range(200):\n",
    "            sampled = np.zeros((1,maxlen,len(allchars)))\n",
    "            for t, char in enumerate(gen_text):\n",
    "                sampled[0,t,charind[char]] = 1\n",
    "            preds = keatsmodel.predict(sampled, verbose=0)[0]\n",
    "            nextind = sample(preds,temp)\n",
    "            newchar = allchars[nextind]\n",
    "            #add new char and move along string\n",
    "            gen_text += newchar\n",
    "            gen_text = gen_text[1:]\n",
    "            sys.stdout.write(newchar)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keatsmodel.save(\"keats_char_gen.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new Keats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keatsmodel = keras.models.load_model(\"keats_char_gen.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " more clear down steeping but the sun \n",
      " while upon my closed and grow not to me \n",
      " made he look'd the fancy bright of the bride \n",
      " when at new and the tender content \n",
      " and the vales sorrow and the eye \n",
      " in the world a song \n",
      " care born and the incense came \n",
      " and while shall be found his hours of self \n",
      " he seem'd in words and e'er ear to be seen \n",
      " phore sat luxurial forest was an footstep \n",
      " was fair god meadow in her green not her late \n",
      " the portal rose and misery in the stars \n",
      " where are ere she see of one i dead \n"
     ]
    }
   ],
   "source": [
    "start_ind = np.random.randint(0,len(corpus)-maxlen-1)\n",
    "gen_text = corpus[start_ind:start_ind+maxlen]\n",
    "temp = 0.1\n",
    "exciteT = 0.7 #applied on first letter of a word to reduce repetition\n",
    "exciteT2 = 0.5 #applied on the second letters of a word to reduce repetition\n",
    "exciteT3 = 0.3 #applied on third letter of a word to reduce repetition\n",
    "exciteT4 = 0.2 #applied on fourth letter of a word to reduce repetition\n",
    "\n",
    "runenhance = True\n",
    "continueval = True\n",
    "printon = False\n",
    "i = 0\n",
    "finishline = 500\n",
    "while continueval:\n",
    "    sampled = np.zeros((1,maxlen,len(allchars)))\n",
    "    for t, char in enumerate(gen_text):\n",
    "        sampled[0,t,charind[char]] = 1\n",
    "    preds = keatsmodel.predict(sampled, verbose=0)[0]\n",
    "    enhanceT = 0.0\n",
    "    if runenhance:\n",
    "        if gen_text[-4] == ' ':\n",
    "            enhanceT = exciteT4\n",
    "        if gen_text[-3] == ' ':\n",
    "            enhanceT = exciteT3\n",
    "        if gen_text[-2] == ' ':\n",
    "            enhanceT = exciteT2\n",
    "        if gen_text[-1] == ' ':\n",
    "            enhanceT = exciteT\n",
    "    nextind = sample(preds,temp + enhanceT)\n",
    "    newchar = allchars[nextind]\n",
    "    #add new char and move along string\n",
    "    gen_text += newchar\n",
    "    gen_text = gen_text[1:]\n",
    "    if printon:\n",
    "        sys.stdout.write(newchar)\n",
    "        i+=1\n",
    "    if newchar == '\\n':\n",
    "        printon = True\n",
    "        if i >= finishline:\n",
    "            continueval = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "wordmodel = Word2Vec(split_works_into_words(cleanedworks), min_count=1)\n",
    "print(wordmodel)\n",
    "# summarize vocabulary\n",
    "words = list(wordmodel.wv.vocab)\n",
    "print(words)\n",
    "print(wordmodel['crept'])\n",
    "# save model\n",
    "wordmodel.save('wordmodel.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wordmodel[wordmodel.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(wordmodel.wv.vocab)\n",
    "#for i, word in enumerate(words):\n",
    "#    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerE = Embedding(len(words), edim, input_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "new_wordmodel = Word2Vec.load('wordmodel.bin')\n",
    "print(new_wordmodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
